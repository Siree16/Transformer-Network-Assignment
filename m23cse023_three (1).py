# -*- coding: utf-8 -*-
"""M23CSE023_three

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B0A5lZVq3j3V3NLeSb4CctnWbZt0JMtF
"""

!pip install wandb lightning

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import torchaudio
from torchvision.transforms import Compose
import torchaudio.transforms as T
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve, auc
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import wandb
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import itertools



class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads=1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.scaling = self.embed_dim ** -0.5

        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)

        self.out_proj = nn.Linear(embed_dim, embed_dim)

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_heads, self.embed_dim // self.num_heads)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, x):
        q = self.query(x)
        k = self.key(x)
        v = self.value(x)

        q = self.transpose_for_scores(q)
        k = self.transpose_for_scores(k)
        v = self.transpose_for_scores(v)

        attn_weights = torch.matmul(q, k.transpose(-1, -2))
        attn_weights *= self.scaling
        attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1)

        attn_output = torch.matmul(attn_weights, v)
        attn_output = attn_output.transpose(1, 2).reshape(attn_output.size(0), -1, self.embed_dim)
        attn_output = self.out_proj(attn_output)

        return attn_output

class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)

        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim, embed_dim),
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        attn_output = self.attention(x)
        x = x + self.dropout(attn_output)
        x = self.norm1(x)

        ffn_output = self.ffn(x)
        x = x + self.dropout(ffn_output)
        x = self.norm2(x)

        return x

class CustomConvNet(nn.Module):
    def __init__(self, num_classes, num_channels=40, transformer_heads=[1, 2, 4]):
        super(CustomConvNet, self).__init__()

        self.conv = nn.Sequential(
            nn.Conv1d(num_channels, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.MaxPool1d(kernel_size=2, stride=2),

            nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.MaxPool1d(kernel_size=2, stride=2),

            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.MaxPool1d(kernel_size=2, stride=2),
        )

        transformer_blocks = []
        for num_heads in transformer_heads:
            transformer_blocks.append(TransformerBlock(128, num_heads))

        self.transformer = nn.Sequential(*transformer_blocks)

        self.cls_token = nn.Parameter(torch.randn(1, 1, 128))

        self.classifier = nn.Sequential(
            nn.Linear(128, num_classes),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        x = self.conv(x)
        x = x.permute(0, 2, 1)
        cls_token = self.cls_token.expand(x.size(0), -1, -1)
        x = torch.cat([cls_token, x], dim=1)
        x = self.transformer(x)
        x = x.mean(dim=1)
        x = self.classifier(x)
        return x

def preprocess_audio(batch, sample_rate=44100):
    waveforms = []
    targets = []

    for audio_file, target in batch:
        waveform, _ = torchaudio.load(audio_file, normalize=True)

        transform = Compose([
            T.Resample(orig_freq=sample_rate, new_freq=16000),
            T.MFCC(),
            T.TimeMasking(time_mask_param=20),
            T.FrequencyMasking(freq_mask_param=30)
        ])

        processed_waveform = transform(waveform)
        waveforms.append(processed_waveform)
        targets.append(target)

    return torch.stack(waveforms).squeeze(1), torch.tensor(targets)

def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, wandb_log=True):
    best_val_loss = float('inf')
    patience = 5
    epochs_no_improve = 0

    if wandb_log:
        wandb.watch(model, log='all')

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct_predictions = 0
        total_samples = 0

        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == targets).sum().item()
            total_samples += targets.size(0)

        train_loss = running_loss / len(train_loader)
        train_accuracy = correct_predictions / total_samples

        # Validation
        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, device)

        if wandb_log:
            wandb.log({'epoch': epoch + 1, 'train_loss': train_loss, 'train_accuracy': train_accuracy,
                       'val_loss': val_loss, 'val_accuracy': val_accuracy})

        print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, "
              f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")

    print("Training completed.")

def evaluate_model(model, test_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0

    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            running_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == targets).sum().item()
            total_samples += targets.size(0)

    loss = running_loss / len(test_loader)
    accuracy = correct_predictions / total_samples

    return loss, accuracy

def count_parameters(model):
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    non_trainable_params = total_params - trainable_params
    return total_params, trainable_params, non_trainable_params

# Function to plot accuracy and loss per epoch
def plot_metrics(epochs, train_loss, val_loss, train_accuracy, val_accuracy):
    plt.figure(figsize=(10, 5))
    plt.plot(epochs, train_loss, label='Train Loss', color='blue')
    plt.plot(epochs, val_loss, label='Validation Loss', color='orange')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.show()

    plt.figure(figsize=(10, 5))
    plt.plot(epochs, train_accuracy, label='Train Accuracy', color='blue')
    plt.plot(epochs, val_accuracy, label='Validation Accuracy', color='orange')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    plt.show()


def evaluate_metrics(model, test_loader, device):
    model.eval()
    all_predictions = []
    all_targets = []
    all_probs = []

    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

            probs = torch.nn.functional.softmax(outputs, dim=1)
            all_probs.extend(probs.cpu().numpy())

    accuracy = accuracy_score(all_targets, all_predictions)
    confusion_mat = confusion_matrix(all_targets, all_predictions)
    f1 = f1_score(all_targets, all_predictions, average='macro')

    roc_auc = roc_auc_score(all_targets, np.array(all_probs), multi_class='ovr')

    return accuracy, confusion_mat, f1, roc_auc


def plot_roc_curve(y_true, y_score, num_classes):
    plt.figure(figsize=(10, 8))
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(num_classes):
        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_score[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
        plt.plot(fpr[i], tpr[i], lw=2, label='Class %d (AUC = %0.2f)' % (i, roc_auc[i]))

    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.show()


def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):
    plt.figure(figsize=(10, 8))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

def main():
    path = '/content/drive/MyDrive/Archive/audio'
    df = pd.read_csv('/content/drive/MyDrive/Archive/meta/esc50.csv')

    esc_10_flag = True
    if esc_10_flag:
        df = df[df['esc10'] == True]

    wandb.init(project='finalll', name='missclassyfyed', config={'epochs': 100, 'learning_rate': 0.001})

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    label_encoder = LabelEncoder()
    df['target'] = label_encoder.fit_transform(df['category'])
    num_classes = len(label_encoder.classes_)


    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
    train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)

    train_loader = DataLoader([(Path(path) / filename, target) for filename, target in zip(train_df['filename'], train_df['target'])], batch_size=32, shuffle=True, collate_fn=preprocess_audio)
    val_loader = DataLoader([(Path(path) / filename, target) for filename, target in zip(val_df['filename'], val_df['target'])], batch_size=32, shuffle=False, collate_fn=preprocess_audio)
    test_loader = DataLoader([(Path(path) / filename, target) for filename, target in zip(test_df['filename'], test_df['target'])], batch_size=32, shuffle=False, collate_fn=preprocess_audio)

    model = CustomConvNet(num_classes=num_classes, num_channels=40)
    model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, device=device)

    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)
    print("Test Loss:", test_loss)
    print("Test Accuracy:", test_accuracy)

    total_params, trainable_params, non_trainable_params = count_parameters(model)
    print(f"Total Trainable Parameters: {trainable_params}")
    print(f"Total Non-trainable Parameters: {non_trainable_params}")

    test_accuracy, confusion_mat, f1, roc_auc = evaluate_metrics(model, test_loader, device)
    print("Test Accuracy:", test_accuracy)
    print("Confusion Matrix:\n", confusion_mat)
    print("F1 Score:", f1)
    print("AUC-ROC Score:", roc_auc)


    all_targets = []
    all_probs = []
    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)

            probs = torch.nn.functional.softmax(outputs, dim=1)
            all_probs.extend(probs.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    all_targets = np.array(all_targets)
    all_probs = np.array(all_probs)


    all_targets_one_hot = label_binarize(all_targets, classes=np.arange(num_classes))


    plot_roc_curve(all_targets_one_hot, all_probs, num_classes)


    plot_confusion_matrix(confusion_mat, classes=label_encoder.classes_)

    wandb.finish()

if __name__ == "__main__":
    main()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import torchaudio
from torchvision.transforms import Compose
import torchaudio.transforms as T
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve, auc
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import wandb
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import itertools



class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads=1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.scaling = self.embed_dim ** -0.5

        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)

        self.out_proj = nn.Linear(embed_dim, embed_dim)

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_heads, self.embed_dim // self.num_heads)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, x):
        q = self.query(x)
        k = self.key(x)
        v = self.value(x)

        q = self.transpose_for_scores(q)
        k = self.transpose_for_scores(k)
        v = self.transpose_for_scores(v)

        attn_weights = torch.matmul(q, k.transpose(-1, -2))
        attn_weights *= self.scaling
        attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1)

        attn_output = torch.matmul(attn_weights, v)
        attn_output = attn_output.transpose(1, 2).reshape(attn_output.size(0), -1, self.embed_dim)
        attn_output = self.out_proj(attn_output)

        return attn_output

class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)

        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim, embed_dim),
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        attn_output = self.attention(x)
        x = x + self.dropout(attn_output)
        x = self.norm1(x)

        ffn_output = self.ffn(x)
        x = x + self.dropout(ffn_output)
        x = self.norm2(x)

        return x

class CustomConvNet(nn.Module):
    def __init__(self, num_classes, num_channels=40, transformer_heads=[1, 2, 4]):
        super(CustomConvNet, self).__init__()

        self.conv = nn.Sequential(
            nn.Conv1d(num_channels, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.MaxPool1d(kernel_size=2, stride=2),

            nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.MaxPool1d(kernel_size=2, stride=2),

            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.MaxPool1d(kernel_size=2, stride=2),
        )

        transformer_blocks = []
        for num_heads in transformer_heads:
            transformer_blocks.append(TransformerBlock(128, num_heads))

        self.transformer = nn.Sequential(*transformer_blocks)

        self.cls_token = nn.Parameter(torch.randn(1, 1, 128))

        self.classifier = nn.Sequential(
            nn.Linear(128, num_classes),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        x = self.conv(x)
        x = x.permute(0, 2, 1)
        cls_token = self.cls_token.expand(x.size(0), -1, -1)
        x = torch.cat([cls_token, x], dim=1)
        x = self.transformer(x)
        x = x.mean(dim=1)
        x = self.classifier(x)
        return x

def preprocess_audio(batch, sample_rate=44100):
    waveforms = []
    targets = []

    for audio_file, target in batch:
        waveform, _ = torchaudio.load(audio_file, normalize=True)

        transform = Compose([
            T.Resample(orig_freq=sample_rate, new_freq=16000),
            T.MFCC(),
            T.TimeMasking(time_mask_param=20),
            T.FrequencyMasking(freq_mask_param=30)
        ])

        processed_waveform = transform(waveform)
        waveforms.append(processed_waveform)
        targets.append(target)

    return torch.stack(waveforms).squeeze(1), torch.tensor(targets)

def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, wandb_log=True):
    best_val_loss = float('inf')
    patience = 5
    epochs_no_improve = 0

    if wandb_log:
        wandb.watch(model, log='all')

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct_predictions = 0
        total_samples = 0

        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == targets).sum().item()
            total_samples += targets.size(0)

        train_loss = running_loss / len(train_loader)
        train_accuracy = correct_predictions / total_samples

        # Validation
        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, device)

        if wandb_log:
            wandb.log({'epoch': epoch + 1, 'train_loss': train_loss, 'train_accuracy': train_accuracy,
                       'val_loss': val_loss, 'val_accuracy': val_accuracy})

        print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, "
              f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")

    print("Training completed.")

def evaluate_model(model, test_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0

    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            running_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == targets).sum().item()
            total_samples += targets.size(0)

    loss = running_loss / len(test_loader)
    accuracy = correct_predictions / total_samples

    return loss, accuracy

def count_parameters(model):
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    non_trainable_params = total_params - trainable_params
    return total_params, trainable_params, non_trainable_params

# Function to plot accuracy and loss per epoch
def plot_metrics(epochs, train_loss, val_loss, train_accuracy, val_accuracy):
    plt.figure(figsize=(10, 5))
    plt.plot(epochs, train_loss, label='Train Loss', color='blue')
    plt.plot(epochs, val_loss, label='Validation Loss', color='orange')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.show()

    plt.figure(figsize=(10, 5))
    plt.plot(epochs, train_accuracy, label='Train Accuracy', color='blue')
    plt.plot(epochs, val_accuracy, label='Validation Accuracy', color='orange')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    plt.show()


def evaluate_metrics(model, test_loader, device):
    model.eval()
    all_predictions = []
    all_targets = []
    all_probs = []

    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

            probs = torch.nn.functional.softmax(outputs, dim=1)
            all_probs.extend(probs.cpu().numpy())

    accuracy = accuracy_score(all_targets, all_predictions)
    confusion_mat = confusion_matrix(all_targets, all_predictions)
    f1 = f1_score(all_targets, all_predictions, average='macro')

    roc_auc = roc_auc_score(all_targets, np.array(all_probs), multi_class='ovr')

    return accuracy, confusion_mat, f1, roc_auc


def plot_roc_curve(y_true, y_score, num_classes):
    plt.figure(figsize=(10, 8))
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(num_classes):
        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_score[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
        plt.plot(fpr[i], tpr[i], lw=2, label='Class %d (AUC = %0.2f)' % (i, roc_auc[i]))

    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.show()


def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):
    plt.figure(figsize=(10, 8))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

def main():
    path = '/content/drive/MyDrive/Archive/audio'
    df = pd.read_csv('/content/drive/MyDrive/Archive/meta/esc50.csv')

    esc_10_flag = True
    if esc_10_flag:
        df = df[df['esc10'] == True]

    wandb.init(project='finalll', name='missclassyfyed', config={'epochs': 100, 'learning_rate': 0.001})

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    label_encoder = LabelEncoder()
    df['target'] = label_encoder.fit_transform(df['category'])
    num_classes = len(label_encoder.classes_)


    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
    train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)

    train_loader = DataLoader([(Path(path) / filename, target) for filename, target in zip(train_df['filename'], train_df['target'])], batch_size=32, shuffle=True, collate_fn=preprocess_audio)
    val_loader = DataLoader([(Path(path) / filename, target) for filename, target in zip(val_df['filename'], val_df['target'])], batch_size=32, shuffle=False, collate_fn=preprocess_audio)
    test_loader = DataLoader([(Path(path) / filename, target) for filename, target in zip(test_df['filename'], test_df['target'])], batch_size=32, shuffle=False, collate_fn=preprocess_audio)

    model = CustomConvNet(num_classes=num_classes, num_channels=40)
    model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, device=device)

    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)
    print("Test Loss:", test_loss)
    print("Test Accuracy:", test_accuracy)

    total_params, trainable_params, non_trainable_params = count_parameters(model)
    print(f"Total Trainable Parameters: {trainable_params}")
    print(f"Total Non-trainable Parameters: {non_trainable_params}")

    test_accuracy, confusion_mat, f1, roc_auc = evaluate_metrics(model, test_loader, device)
    print("Test Accuracy:", test_accuracy)
    print("Confusion Matrix:\n", confusion_mat)
    print("F1 Score:", f1)
    print("AUC-ROC Score:", roc_auc)


    all_targets = []
    all_probs = []
    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)

            probs = torch.nn.functional.softmax(outputs, dim=1)
            all_probs.extend(probs.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    all_targets = np.array(all_targets)
    all_probs = np.array(all_probs)


    all_targets_one_hot = label_binarize(all_targets, classes=np.arange(num_classes))


    plot_roc_curve(all_targets_one_hot, all_probs, num_classes)


    plot_confusion_matrix(confusion_mat, classes=label_encoder.classes_)

    wandb.finish()

if __name__ == "__main__":
    main()

